import math
import random
import string

random.seed(0)

# 生成区间[a, b)内的随机数
def rand(a, b):
    return (b-a)*random.random() + a

# 生成大小 I*J 的矩阵，默认零矩阵 (当然，亦可用 NumPy 提速)
def makeMatrix(I, J, fill=0.0):
    m = []
    for i in range(I):
        m.append([fill]*J)
    return m

# 函数 sigmoid，这里采用 tanh，因为看起来要比标准的 1/(1+e^-x) 漂亮些
def sigmoid(x):
    return math.tanh(x)

# 函数 sigmoid 的派生函数, 为了得到输出 (即：y)
def dsigmoid(y):
    return 1.0 - y**2

class NN:
    ''' 三层反向传播神经网络 '''
    def __init__(self, ni, nh, no):
        # 输入层、隐藏层、输出层的节点（数）
        self.ni = ni + 1 # 增加一个偏差节点
        self.nh = nh
        self.no = no

        # 激活神经网络的所有节点（向量）
        self.ai = [1.0]*self.ni
        self.ah = [1.0]*self.nh
        self.ao = [1.0]*self.no
        
        # 建立权重（矩阵）
        self.wi = makeMatrix(self.ni, self.nh)
        self.wo = makeMatrix(self.nh, self.no)
        # 设为随机值
        for i in range(self.ni):
            for j in range(self.nh):
                self.wi[i][j] = rand(-0.1, 0.1)
        for j in range(self.nh):
            for k in range(self.no):
                self.wo[j][k] = rand(-1.0, 1.0)

        # 最后建立动量因子（矩阵）
        self.ci = makeMatrix(self.ni, self.nh)
        self.co = makeMatrix(self.nh, self.no)

    def update(self, inputs):
        if len(inputs) != self.ni-1:
            raise ValueError('与输入层节点数不符！')

        # 激活输入层
        for i in range(self.ni-1):
            #self.ai[i] = sigmoid(inputs[i])
            self.ai[i] = inputs[i]

        # 激活隐藏层
        for j in range(self.nh):
            sum = 0.0
            for i in range(self.ni):
                sum = sum + self.ai[i] * self.wi[i][j]
            self.ah[j] = sigmoid(sum)

        # 激活输出层
        for k in range(self.no):
            sum = 0.0
            for j in range(self.nh):
                sum = sum + self.ah[j] * self.wo[j][k]
            self.ao[k] = sigmoid(sum)

        return self.ao[:]

    def backPropagate(self, targets, N, M):
        ''' 反向传播 '''
        if len(targets) != self.no:
            raise ValueError('与输出层节点数不符！')

        # 计算输出层的误差
        output_deltas = [0.0] * self.no
        for k in range(self.no):
            error = targets[k]-self.ao[k]
            output_deltas[k] = dsigmoid(self.ao[k]) * error

        # 计算隐藏层的误差
        hidden_deltas = [0.0] * self.nh
        for j in range(self.nh):
            error = 0.0
            for k in range(self.no):
                error = error + output_deltas[k]*self.wo[j][k]
            hidden_deltas[j] = dsigmoid(self.ah[j]) * error

        # 更新输出层权重
        for j in range(self.nh):
            for k in range(self.no):
                change = output_deltas[k]*self.ah[j]
                self.wo[j][k] = self.wo[j][k] + N*change + M*self.co[j][k]
                self.co[j][k] = change
                #print(N*change, M*self.co[j][k])

        # 更新输入层权重
        for i in range(self.ni):
            for j in range(self.nh):
                change = hidden_deltas[j]*self.ai[i]
                self.wi[i][j] = self.wi[i][j] + N*change + M*self.ci[i][j]
                self.ci[i][j] = change

        # 计算误差
        error = 0.0
        for k in range(len(targets)):
            error = error + 0.5*(targets[k]-self.ao[k])**2
        return error

    def test(self, patterns):
        for p in patterns:
            print(p[0], '->', self.update(p[0]))

    def weights(self):
        print('输入层权重:')
        for i in range(self.ni):
            print(self.wi[i])
        print()
        print('输出层权重:')
        for j in range(self.nh):
            print(self.wo[j])

    def train(self, patterns, iterations=5000, N=0.5, M=0.1):
        # N: 学习速率(learning rate)
        # M: 动量因子(momentum factor)
        for i in range(iterations):
            error = 0.0
            for p in patterns:
                inputs = p[0]
                targets = p[1]
                self.update(inputs)
                error = error + self.backPropagate(targets, N, M)
            if i % 1000 == 0:
                print('误差 %-.5f' % error)

    def predict(self, patterns):
        for p in patterns:
            print(p[0], '->', self.update(p[0]))


def demo():
    # 一个演示：教神经网络学习逻辑异或（XOR）------------可以换成你自己的数据试试


    pat_bk = [
     [[5.03/100,6.55/100,8.75/100,8.37/100,6.79/100,7.60/100,9.71/100,9.61/100,7.41/100,7.49/100,6.32/100,4.42/100,3.62/100,3.29/100,2.46/100,1.47/100,0.75/100,0.27/100,0.08/100,0.02/100], [9.41/100]],
     [[4.94/100,6.08/100,8.28/100,8.72/100,6.35/100,7.06/100,9.41/100,9.78/100,8.03/100,7.15/100,6.9 /100,4.93/100,3.8 /100,3.2 /100,2.6 /100,1.53/100,0.84/100,0.3 /100,0.09/100,0.02/100], [8.51/100]],
     [[5.34/100,6.24/100,7.97/100,8.5 /100,6.1 /100,6.54/100,8.51/100,9.72/100,8.69/100,6.76/100,7.28/100,5.34/100,3.93/100,3.32/100,2.68/100,1.71/100,0.92/100,0.33/100,0.09/100,0], [7.83/100]],
     [[5.08/100,5.92/100,7.47/100,8.81/100,6.39/100,6.21/100,7.83/100,9.55/100,9.71/100,6.41/100,7.60/100,5.72/100,4.10/100,3.35/100,2.74/100,1.73/100,0.91/100,0.35/100,0,0], [7.3/100]],
     [[5.05/100,5.74/100,7.09/100,8.32/100,6.61/100,6.46/100,7.3 /100,9.71/100,9.75/100,6.4 /100,7.78/100,6.16/100,4.29/100,3.35/100,2.8 /100,1.77/100,0.95/100,0.37/100,0.09/100,0.02/100], [6.96/100]],
     [[5.13/100,5.46/100,6.73/100,7.87/100,6.86/100,6.48/100,6.96/100,9.27/100,9.65/100,7.19/100,7.8 /100,6.58/100,4.47/100,3.32/100,2.83/100,1.87/100,0.99/100,0.41/100,0.1 /100,0.02/100], [6.76/100]],
     [[5.16/100,5.41/100,6.3 /100,7.17/100,7.52/100,6.48/100,6.76/100,9.1 /100,9.64/100,7.93/100,7.24/100,6.79/100,4.78/100,3.44/100,2.79/100,1.93/100,1.01/100,0.41/100,0.1 /100,0.03/100], [7.29/100]],
     [[5.67/100,5.32/100,5.62/100,7.49/100,9.56/100,7.58/100,7.29/100,8.86/100,9.36/100,7.92/100,5.91/100,6.1 /100,4.4 /100,3.08/100,2.47/100,1.79/100,1   /100,0.42/100,0.12/100,0.03/100], [7.17/100]],
     [[5.66/100,5.35/100,5.46/100,7.02/100,9.48/100,7.79/100,7.17/100,8.46/100,9.38/100,8.81/100,5.43/100,6.26/100,4.6 /100,3.15/100,2.52/100,1.85/100,1.04/100,0.42/100,0.13/100,0.03/100], [7.43/100]],
     [[5.69/100,5.45/100,5.32/100,6.57/100,9.05/100,8.00/100,7.43/100,7.92/100,9.56/100,8.83/100,5.51/100,6.35/100,4.93/100,3.34/100,2.51/100,1.89/100,1.08/100,0.42/100,0.13/100,0.03/100], [7.39/100]],
     [[5.69/100,5.45/100,5.32/100,6.57/100,9.05/100,8.00/100,7.43/100,7.92/100,9.56/100,8.83/100,5.51/100,6.35/100,4.93/100,3.34/100,2.51/100,1.89/100,1.08/100,0.42/100,0.13/100,0.03/100], [7.39/100]]
    ]

    pat = [
     [[5.03/100,6.55/100,8.75/100,8.37/100,6.79/100,7.60/100,9.71/100,9.61/100,7.41/100,7.49/100,6.32/100,4.42/100,3.62/100,3.29/100,2.46/100,1.47/100,0.75/100,0.27/100,0.08/100,0.02/100], [4.94/100,6.08/100,8.28/100,8.72/100,6.35/100,7.06/100,9.41/100,9.78/100,8.03/100,7.15/100,6.90/100,4.93/100,3.80/100,3.20/100,2.60/100,1.53/100,0.84/100,0.30/100,0.09/100,0.02/100]],
     [[4.94/100,6.08/100,8.28/100,8.72/100,6.35/100,7.06/100,9.41/100,9.78/100,8.03/100,7.15/100,6.90/100,4.93/100,3.80/100,3.20/100,2.60/100,1.53/100,0.84/100,0.30/100,0.09/100,0.02/100], [5.34/100,6.24/100,7.97/100,8.50/100,6.10/100,6.54/100,8.51/100,9.72/100,8.69/100,6.76/100,7.28/100,5.34/100,3.93/100,3.32/100,2.68/100,1.71/100,0.92/100,0.33/100,0.09/100,0.00/100]],
     [[5.34/100,6.24/100,7.97/100,8.50/100,6.10/100,6.54/100,8.51/100,9.72/100,8.69/100,6.76/100,7.28/100,5.34/100,3.93/100,3.32/100,2.68/100,1.71/100,0.92/100,0.33/100,0.09/100,0.00/100], [5.08/100,5.92/100,7.47/100,8.81/100,6.39/100,6.21/100,7.83/100,9.55/100,9.71/100,6.41/100,7.60/100,5.72/100,4.10/100,3.35/100,2.74/100,1.73/100,0.91/100,0.35/100,0.00/100,0.00/100]],
     [[5.08/100,5.92/100,7.47/100,8.81/100,6.39/100,6.21/100,7.83/100,9.55/100,9.71/100,6.41/100,7.60/100,5.72/100,4.10/100,3.35/100,2.74/100,1.73/100,0.91/100,0.35/100,0.00/100,0.00/100], [5.05/100,5.74/100,7.09/100,8.32/100,6.61/100,6.46/100,7.30/100,9.71/100,9.75/100,6.40/100,7.78/100,6.16/100,4.29/100,3.35/100,2.80/100,1.77/100,0.95/100,0.37/100,0.09/100,0.02/100]],
     [[5.05/100,5.74/100,7.09/100,8.32/100,6.61/100,6.46/100,7.30/100,9.71/100,9.75/100,6.40/100,7.78/100,6.16/100,4.29/100,3.35/100,2.80/100,1.77/100,0.95/100,0.37/100,0.09/100,0.02/100], [5.13/100,5.46/100,6.73/100,7.87/100,6.86/100,6.48/100,6.96/100,9.27/100,9.65/100,7.19/100,7.80/100,6.58/100,4.47/100,3.32/100,2.83/100,1.87/100,0.99/100,0.41/100,0.10/100,0.02/100]],
     [[5.13/100,5.46/100,6.73/100,7.87/100,6.86/100,6.48/100,6.96/100,9.27/100,9.65/100,7.19/100,7.80/100,6.58/100,4.47/100,3.32/100,2.83/100,1.87/100,0.99/100,0.41/100,0.10/100,0.02/100], [5.16/100,5.41/100,6.30/100,7.17/100,7.52/100,6.48/100,6.76/100,9.10/100,9.64/100,7.93/100,7.24/100,6.79/100,4.78/100,3.44/100,2.79/100,1.93/100,1.01/100,0.41/100,0.10/100,0.03/100]],
     [[5.16/100,5.41/100,6.30/100,7.17/100,7.52/100,6.48/100,6.76/100,9.10/100,9.64/100,7.93/100,7.24/100,6.79/100,4.78/100,3.44/100,2.79/100,1.93/100,1.01/100,0.41/100,0.10/100,0.03/100], [5.67/100,5.32/100,5.62/100,7.49/100,9.56/100,7.58/100,7.29/100,8.86/100,9.36/100,7.92/100,5.91/100,6.10/100,4.40/100,3.08/100,2.47/100,1.79/100,1.00/100,0.42/100,0.12/100,0.03/100]],
     [[5.67/100,5.32/100,5.62/100,7.49/100,9.56/100,7.58/100,7.29/100,8.86/100,9.36/100,7.92/100,5.91/100,6.10/100,4.40/100,3.08/100,2.47/100,1.79/100,1.00/100,0.42/100,0.12/100,0.03/100], [5.66/100,5.35/100,5.46/100,7.02/100,9.48/100,7.79/100,7.17/100,8.46/100,9.38/100,8.81/100,5.43/100,6.26/100,4.60/100,3.15/100,2.52/100,1.85/100,1.04/100,0.42/100,0.13/100,0.03/100]],
     [[5.66/100,5.35/100,5.46/100,7.02/100,9.48/100,7.79/100,7.17/100,8.46/100,9.38/100,8.81/100,5.43/100,6.26/100,4.60/100,3.15/100,2.52/100,1.85/100,1.04/100,0.42/100,0.13/100,0.03/100], [5.69/100,5.45/100,5.32/100,6.57/100,9.05/100,8.00/100,7.43/100,7.92/100,9.56/100,8.83/100,5.51/100,6.35/100,4.93/100,3.34/100,2.51/100,1.89/100,1.08/100,0.42/100,0.13/100,0.03/100]],
     [[5.69/100,5.45/100,5.32/100,6.57/100,9.05/100,8.00/100,7.43/100,7.92/100,9.56/100,8.83/100,5.51/100,6.35/100,4.93/100,3.34/100,2.51/100,1.89/100,1.08/100,0.42/100,0.13/100,0.03/100], [5.68/100,5.58/100,5.15/100,6.14/100,8.71/100,8.33/100,7.39/100,7.54/100,9.28/100,8.77/100,6.22/100,6.32/100,5.21/100,3.51/100,2.49/100,1.90/100,1.14/100,0.47/100,0.14/100,0.03/100]],
     [[5.68/100,5.58/100,5.15/100,6.14/100,8.71/100,8.33/100,7.39/100,7.54/100,9.28/100,8.77/100,6.22/100,6.32/100,5.21/100,3.51/100,2.49/100,1.90/100,1.14/100,0.47/100,0.14/100,0.03/100], [5.69/100,5.61/100,5.18/100,5.76/100,8.07/100,8.79/100,7.34/100,7.27/100,9.07/100,8.83/100,6.93/100,5.91/100,5.48/100,3.71/100,2.59/100,1.90/100,1.18/100,0.50/100,0.16/100,0.03/100]],
     [[5.69/100,5.61/100,5.18/100,5.76/100,8.07/100,8.79/100,7.34/100,7.27/100,9.07/100,8.83/100,6.93/100,5.91/100,5.48/100,3.71/100,2.59/100,1.90/100,1.18/100,0.50/100,0.16/100,0.03/100], [5.83/100,5.51/100,5.18/100,5.47/100,7.30/100,9.35/100,7.38/100,7.07/100,8.56/100,9.01/100,7.58/100,5.60/100,5.68/100,3.99/100,2.64/100,1.93/100,1.19/100,0.52/100,0.16/100,0.03/100]],
     [[5.83/100,5.51/100,5.18/100,5.47/100,7.30/100,9.35/100,7.38/100,7.07/100,8.56/100,9.01/100,7.58/100,5.60/100,5.68/100,3.99/100,2.64/100,1.93/100,1.19/100,0.52/100,0.16/100,0.03/100], [6.91/100,5.51/100,5.22/100,5.32/100,6.83/100,9.21/100,7.56/100,6.95/100,8.18/100,9.03/100,8.43/100,5.15/100,5.85/100,4.18/100,2.74/100,1.94/100,1.24/100,0.66/100,0.16/100,0.04/100]],

     [[6.91/100,5.51/100,5.22/100,5.32/100,6.83/100,9.21/100,7.56/100,6.95/100,8.18/100,9.03/100,8.43/100,5.15/100,5.85/100,4.18/100,2.74/100,1.94/100,1.24/100,0.66/100,0.16/100,0.04/100], [6.76/100,5.43/100,5.35/100,5.21/100,5.90/100,9.35/100,7.59/100,7.13/100,8.11/100,8.97/100,9.25/100,4.80/100,6.14/100,4.36/100,2.72/100,2.03/100,1.35/100,0.81/100,0.20/100,0.04/100]],
     [[6.76/100,5.43/100,5.35/100,5.21/100,5.90/100,9.35/100,7.59/100,7.13/100,8.11/100,8.97/100,9.25/100,4.80/100,6.14/100,4.36/100,2.72/100,2.03/100,1.35/100,0.81/100,0.20/100,0.04/100], [7.09/100,5.35/100,5.58/100,5.15/100,5.13/100,9.34/100,7.50/100,7.25/100,7.92/100,8.92/100,9.94/100,4.55/100,6.26/100,4.59/100,2.79/100,2.20/100,1.39/100,0.95/100,0.25/100,0.05/100]],
     [[7.09/100,5.35/100,5.58/100,5.15/100,5.13/100,9.34/100,7.50/100,7.25/100,7.92/100,8.92/100,9.94/100,4.55/100,6.26/100,4.59/100,2.79/100,2.20/100,1.39/100,0.95/100,0.25/100,0.05/100], [7.29/100,5.30/100,5.74/100,5.23/100,4.55/100,9.19/100,7.51/100,7.47/100,7.87/100,8.83/100,10.52/100,4.32/100,6.37/100,4.76/100,2.85/100,2.30/100,1.45/100,1.08/100,0.27/100,0.05/100]],
     [[7.29 / 100, 5.30 / 100, 5.74 / 100, 5.23 / 100, 4.55 / 100, 9.19 / 100, 7.51 / 100,7.47 / 100, 7.87 / 100, 8.83 / 100, 10.52 / 100, 4.32 / 100, 6.37 / 100, 4.76 / 100,2.85 / 100, 2.30 / 100, 1.45 / 100, 1.08 / 100, 0.27 / 100, 0.05 / 100], [7.41/100,5.25/100,5.92/100,5.34/100,4.09/100,9.02/100,7.45/100,7.71/100,7.87/100,8.68/100,10.94/100,4.19/100,6.41/100,4.84/100,2.90/100,2.36/100,1.48/100,1.16/100,0.30/100,0.06/100]],
    # 2020 predicts 2021
     [[7.41/100,5.25/100,5.92/100,5.34/100,4.09/100,9.02/100,7.45/100,7.71/100,7.87/100,8.68/100,10.94/100,4.19/100,6.41/100,4.84/100,2.90/100,2.36/100,1.48/100,1.16/100,0.30/100,0.06/100], [7.47/100,5.20/100,6.07/100,5.51/100,3.81/100,8.81/100,7.37/100,7.95/100,7.92/100,8.54/100,11.20/100,4.15/100,6.40/100,4.89/100,2.94/100,2.41/100,1.50/100,1.22/100,0.31/100,0.06/100]],
    # 2021 predicts 2022
     [[7.47/100,5.20/100,6.07/100,5.51/100,3.81/100,8.81/100,7.37/100,7.95/100,7.92/100,8.54/100,11.20/100,4.15/100,6.40/100,4.89/100,2.94/100,2.41/100,1.50/100,1.22/100,0.31/100,0.06/100], [7.47/100,5.16/100,6.18/100,5.68/100,3.70/100,8.58/100,7.29/100,8.18/100,8.02/100,8.42/100,11.29/100,4.18/100,6.35/100,4.89/100,2.97/100,2.44/100,1.50/100,1.26/100,0.32/100,0.06/100]],
    # 2022 predicts 2023
     [[7.47/100,5.16/100,6.18/100,5.68/100,3.70/100,8.58/100,7.29/100,8.18/100,8.02/100,8.42/100,11.29/100,4.18/100,6.35/100,4.89/100,2.97/100,2.44/100,1.50/100,1.26/100,0.32/100,0.06/100], [7.41/100,5.13/100,6.24/100,5.86/100,3.76/100,8.37/100,7.22/100,8.37/100,8.15/100,8.32/100,11.22/100,4.27/100,6.27/100,4.84/100,2.98/100,2.46/100,1.50/100,1.26/100,0.33/100,0.07/100]],
    # 2023 --> 2024
     [[7.41/100,5.13/100,6.24/100,5.86/100,3.76/100,8.37/100,7.22/100,8.37/100,8.15/100,8.32/100,11.22/100,4.27/100,6.27/100,4.84/100,2.98/100,2.46/100,1.50/100,1.26/100,0.33/100,0.07/100], [7.31/100,5.11/100,6.26/100,6.01/100,3.92/100,8.19/100,7.15/100,8.50/100,8.30/100,8.26/100,11.04/100,4.40/100,6.17/100,4.76/100,2.98/100,2.46/100,1.48/100,1.24/100,0.32/100,0.06/100]]
    ]

    #predictPat = [[[6.91/100,5.51/100,5.22/100,5.32/100,6.83/100,9.21/100,7.56/100,6.95/100,8.18/100,9.03/100,8.43/100,5.15/100,5.85/100,4.18/100,2.74/100,1.94/100,1.24/100,0.66/100,0.16/100,0.04/100], [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]]
    #predictPat = [[[7.18/100,5.33/100,5.73/100,5.10/100,5.92/100,9.27/100,7.63/100,7.22/100,8.05/100,9.01/100,9.28/100,4.71/100,6.12/100,4.26/100,2.93/100,1.92/100,1.46/100,0.60/100,0.15/100,0.07/100], [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]]
    #predictPat = [[[7.32/100,5.12/100,6.26/100,6.01/100,3.94/100,8.19/100,7.15/100,8.50/100,8.30/100,8.24/100,11.02/100,4.40/100,6.17/100,4.76/100,2.98/100,2.45/100,1.48/100,1.24/100,0.33/100,0.07/100], [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]]
    # 2023 --> 2024
    #predictPat = [[[7.41/100,5.13/100,6.24/100,5.86/100,3.76/100,8.37/100,7.22/100,8.37/100,8.15/100,8.32/100,11.22/100,4.27/100,6.27/100,4.84/100,2.98/100,2.46/100,1.50/100,1.26/100,0.33/100,0.07/100],[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]
    # 2024 --> 2025
    #predictPat = [[[7.31/100,5.11/100,6.26/100,6.01/100,3.92/100,8.19/100,7.15/100,8.50/100,8.30/100,8.26/100,11.04/100,4.40/100,6.17/100,4.76/100,2.98/100,2.46/100,1.48/100,1.24/100,0.32/100,0.06/100],[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]

    predictPat = [[[6.76/100,5.17/100,5.97/100,6.26/100,5.46/100,8.00/100,7.04/100,8.46/100,8.77/100,8.30/100,9.57/100,5.11/100,5.77/100,4.33/100,2.88/100,2.29/100,1.36/100,1.00/100,0.28/100,0.05/100],[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]


    # 创建一个神经网络：输入层有两个节点、隐藏层有两个节点、输出层有一个节点
    #n = NN(20, 20, 20)
    n = NN(20, 20, 20)
    # 用一些模式训练它
    n.train(pat)
    # 测试训练的成果（不要吃惊哦）
    n.test(pat)
    # 看看训练好的权重（当然可以考虑把训练好的权重持久化）
    n.weights()
    n.predict(predictPat)
    
    
if __name__ == '__main__':
    demo()